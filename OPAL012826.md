OPAL Build Prompt: Agentic AI Document Intelligence System (Streamlit + Gemini/OpenAI)
0) Mission and Success Criteria
Build a production-ready Agentic AI Document Intelligence System that enables regulatory analysts and data stewards to ingest complex documents (primarily PDFs), perform selective trimming and OCR (local and/or Vision LLM), transform extracted content into clean actionable Markdown, and run specialized configurable agents defined in agents.yaml and governed by reusable prompt “skills” in SKILL.md.

The app must feel like a high-polish internal tool: fast to use, resilient to failures, and transparent about processing steps, costs, and sources. It must support both Gemini (Vertex/Google Generative AI) and OpenAI models with a standardized provider interface. It must be deployable on Hugging Face Spaces with secure secret handling.

Primary success criteria

Upload a PDF → preview it → select page ranges → OCR → get cleaned Markdown with headings/tables → highlight regulatory keywords → run an agent → receive structured, cite-aware analysis.
Run an automated “directory ToC pipeline” that discovers PDFs, extracts page 1, summarizes each, and compiles a master ToC.md with links.
Agent system is configurable via agents.yaml and SKILL.md, supports runtime overrides, chaining, and schema-alignment of user-provided YAML.
Keys are never written to disk; environment secrets take precedence, but user can override in UI; keys are masked.
App remains responsive during heavy operations, uses caching appropriately, and handles rate limits and context overflow gracefully.
1) Target Users and Core Use Cases
Personas

Regulatory Analyst: needs rapid triage of many PDFs (adverse event reports, post-market surveillance, recalls), then deep-dive on critical docs; cares about citations, consistency, and traceability.
Data Steward / Knowledge Manager: needs reproducible extraction, normalization to Markdown, standardized summaries, and exportable artifacts (ToC.md, extracted markdown, agent results).
Power User: wants to upload custom agents.yaml and SKILL.md, override prompts/models, chain agents, and batch-process folders.
Use cases

Rapid ingestion & triage: run directory ToC pipeline and get 50 blurbs in minutes; spot critical docs.
Selective extraction: choose “pages 14–16” from a PDF, OCR with Vision LLM to preserve tables, convert to Markdown, highlight “Contraindication / 510(k) / Class II recall”.
Agentic analysis: run “Safety Signal Detector” agent on extracted section; produce risk-ranked summary with citations by page.
Iterative refinement: edit Markdown in-app, rerun different agents, maintain session history, export results.
2) Product Requirements (Features)
2.1 Multi-modal ingestion
Provide three ingestion paths:

Direct paste: large text box where users paste raw text/markdown and proceed to transformation + agent execution.
File upload: .pdf, .txt, .md (multiple allowed, but focus UX on one “active document” at a time).
Directory ToC pipeline: scan working directory for PDFs; summarize first page of each; produce a master ToC.md.
2.2 Enhanced PDF preview
Render PDF in-app (use streamlit-pdf-viewer if available; otherwise implement base64 iframe rendering).
Must support:
page navigation
zoom
ability for user to visually confirm page numbers before trimming/OCR.
2.3 Selective PDF trimming
User enters ranges like "1-5, 10, 15-20".
Parse robustly; validate bounds; show computed list of pages; fail with helpful messages.
Extract selected pages into a temporary PDF (in memory or /tmp), not rewriting original.
Use pypdf or PyMuPDF for extraction; optimize not to load entire document if feasible.
2.4 OCR orchestrator (3 modes)
Provide a single “OCR method” selector with three modes:

Local OCR (Tesseract)

Good for digital-first PDFs or clear scans.
Convert pages to images (via pdf2image/poppler) then run Tesseract.
Return plain text; pass through “Markdown transformation” step.
Vision LLM OCR → Markdown

For complex tables/handwritten/low-quality scans.
Render each selected page to an image and send to Gemini Vision-capable model with an “OCR-to-Markdown” prompt.
Output should be valid Markdown (tables in Markdown table syntax if possible).
Hybrid

Run local OCR first; then send the raw OCR output to an LLM “cleaner” to fix formatting, reconstruct headings/lists/tables, and correct errors.
The app must route intelligently when doing ToC pipeline: if extracted first page contains little/no text, switch to OCR; if it has selectable text, avoid OCR.

2.5 Markdown transformation & semantic reorganization
After extraction/OCR, run a transformation pipeline that:

detects headings, subheadings, bullet lists, numbered lists
reconstructs tables if possible
standardizes whitespace, hyphenation, and line breaks
emits valid Markdown with minimal hallucination (do not invent content)
Include a visible toggle: Raw text vs Rendered Markdown (or side-by-side if screen width allows). Provide an editor area where user can modify the Markdown.

2.6 Keyword highlighting
Maintain a default list of regulatory/clinical keywords (e.g., “Class I”, “Class II”, “510(k)”, “Contraindication”, “Adverse Event”, “Recall”, “Death”, “Permanent Injury”, “Malfunction”).
Let user add custom keywords via UI.
Highlight by wrapping occurrences in: <span style="color:coral">keyword</span>
Ensure highlighting does not corrupt Markdown code blocks or links (apply with care; avoid inside fenced code blocks).
2.7 Agentic framework (agents.yaml + SKILL.md)
Build an agent orchestrator that:

loads agents.yaml
loads SKILL.md
validates/aligns schema for user-supplied agents YAML
allows users to select an agent, edit prompts, override model/temperature/tokens
injects skill snippets into the final system prompt
runs the agent against the current document context (Markdown text in editor)
stores outputs in session history
Schema alignment requirement
If uploaded agents.yaml is missing fields, run a “Schema Alignment Agent” (lightweight, deterministic) that transforms the YAML to the standard schema. The UI must always have id, name, role, default_model, temperature, max_tokens, skills, and system_prompt. If fields are missing:

create stable id from slugified name
apply defaults for model/temperature/tokens
map near-equivalents (e.g., prompt → system_prompt, persona → role)
2.8 Agent chaining
Support a simple chaining mechanism:

user can define a chain (list of agent IDs in order)
output of agent N becomes appended context or the input for agent N+1 (configurable)
provide a “Chain Runner” UI with progress and intermediate outputs
2.9 Automated Batch Intelligence: ToC Pipeline
Implement a pipeline that:

Discovers all .pdf files recursively from a chosen base directory (default current working directory).
Extracts first page from each PDF (streamed extraction; do not load entire file).
If page has text: use it; else OCR it (local or Vision depending on setting).
Summarize into a ~100-word blurb with a fixed prompt defined in agents.yaml (e.g., toc_summarizer agent).
Compile results into a single ToC.md with:
hierarchical listing by folder path
each entry includes file link/name, blurb, and optional extracted metadata
Display ToC in UI and allow download.
Optimize using parallel calls (ThreadPoolExecutor) but do not exceed rate limits; include throttling and backoff.

3) Technical Architecture
3.1 High-level modules
Implement as a Streamlit app with modular Python files:

app.py
Entry point; layout; routing between tabs; session state initialization.

core/pdf_processor.py
PDF loading, metadata, page range parsing, page extraction into temp PDF, page-to-image conversion utilities.

core/ocr.py
Local OCR (Tesseract) + Vision OCR orchestrator + hybrid strategy.

core/markdown_transform.py
Cleanup/transformation functions; LLM-assisted restructuring prompt; safety rules (do not hallucinate).

core/highlight.py
Keyword highlighting with Markdown-safe handling.

agents/orchestrator.py
Load/validate/align YAML; load SKILL.md; assemble prompts; agent execution; chaining.

providers/llm.py
Unified interface for Gemini + OpenAI; retries; exponential backoff; token/cost tracking; context window management.

pipelines/toc.py
Directory scan; first-page extraction; summarization; ToC assembly; concurrency controls.

ui/components.py
Reusable UI pieces: PDF preview, settings panels, editors, download buttons, history viewer.

Also include default config files in repo:

agents.yaml (default agents)
SKILL.md (default skill library)
README.md (deployment + usage)
requirements.txt, packages.txt (HF Spaces)
3.2 State management
Use st.session_state for:

active document text (raw + markdown)
extracted OCR text
selected PDF and page range
agent configs currently loaded
user-entered API keys
run history (list of interactions with timestamps, agent ID, model, prompt overrides, output)
ToC pipeline results and logs
Temporary files:

store trimmed PDFs and rendered page images under /tmp/<session_id>/...
cleanup button to remove temporary artifacts
3.3 Provider layer
Implement LLMClient abstraction with methods like:

generate_text(model, system_prompt, user_prompt, temperature, max_tokens)
generate_vision(model, system_prompt, user_prompt, images[], temperature, max_tokens)
Support models (minimum):

gemini-2.5-flash
gemini-3-flash-preview
gpt-4o-mini
Include:

Retry policy: handle 429 and transient 5xx with jittered exponential backoff.
Logging: record model, prompt sizes, response size, elapsed time, errors.
Token/cost estimation: if exact tokens unavailable, estimate length-based; show “approx usage”.
Context window management:

If input exceeds max tokens, apply:
truncation with clear warning, or
automatic pre-summarization step (configurable)
Must never silently drop huge amounts of context without notifying user.
4) Security and API Key Orchestration
4.1 Key precedence rules
Implement priority:

User-provided key in UI (stored in session_state only)
Environment secret (Hugging Face Spaces secrets / env vars)
If missing, disable provider and show call-to-action
Mask keys in UI:

If key sourced from env, show ●●●●●● [System Default]
If user typed key, show masked with last 4 characters (optional), but never print full key.
No keys written to disk; no keys included in logs; scrub keys from exceptions.

4.2 Data handling
User documents may be sensitive; include UI note that OCR via cloud models sends page images/text to provider.
Provide a toggle: “Local-only mode” (disables Vision LLM and cloud summarization; allows only local OCR + local transformations that do not call LLM, if feasible).
Default to secure behavior and clarity.
5) UI/UX Requirements (Streamlit)
5.1 Layout
Use a clear, professional layout:

Sidebar: Global Settings

Provider selection (Gemini/OpenAI/Both)
Model dropdown (context-aware)
Temperature slider
Max tokens numeric input
API keys inputs (OpenAI + Google) with masking
Keyword list editor (add/remove)
Advanced settings expander: backoff settings, concurrency, OCR DPI, caching toggles
Main area: Tabs

Ingest & Preview

uploader (PDF/TXT/MD)
direct paste text area
PDF preview component with page count
“Set as active document”
Trim & OCR

page range input
OCR mode selector (Local / Vision / Hybrid)
run button with progress UI
output: raw extracted text + markdown transform result
Markdown Workspace

editable Markdown text area (source)
rendered preview (markdown render)
keyword highlight toggle on preview
download buttons: .md, .txt, optionally .pdf export
Agents

agent dropdown (from YAML)
agent details panel (role, skills, system prompt editor)
model override dropdown
“Run Agent” button
output viewer (Markdown)
session history list with ability to restore previous outputs
ToC Pipeline

directory selection input (text path; HF may limit browsing; accept relative path)
run ToC pipeline button
progress + results table
generated ToC.md preview and download
Agent Management

upload/download agents.yaml and SKILL.md
show schema validation results
“align schema” action and diff viewer (before/after)
5.2 Responsiveness and performance
Use st.cache_resource for initializing clients and OCR resources.
Use @st.fragment (if available) to update preview areas without rerunning heavy steps.
Use spinners/progress bars for OCR and batch steps.
Ensure large text areas handle big docs; consider splitting display or using expanders.
6) Default Agent and Skill Library Content
6.1 Default agents.yaml
Include at least these agent archetypes:

reg_analyst – regulatory reasoning and extraction
safety_signal_detector – identify safety signals, risk ranking
toc_summarizer – fixed 100-word blurb for ToC pipeline
schema_alignment_agent – aligns user YAML to standard schema (should be deterministic and cautious)
markdown_rewriter – transforms OCR/raw text into clean Markdown (non-hallucinatory)
Example schema (must implement fully, can expand):

agents:
  - name: "Regulatory Analyst"
    id: "reg_analyst"
    role: "Regulatory logic and structured reasoning"
    default_model: "gemini-2.5-flash"
    temperature: 0.2
    max_tokens: 12000
    skills: ["citation_style", "executive_brief", "data_quality"]
    system_prompt: |
      You are an expert regulatory affairs specialist.
      Parse complex documents and extract actionable, auditable insights.
      Do not invent facts. If uncertain, say so.
6.2 Default SKILL.md
Create a skills library with clearly delineated sections. Each skill is a snippet the orchestrator can inject into system prompt context.

At minimum:

citation_style: define how to cite sources (PDF filename + page numbers; if unknown page, say unknown)
executive_brief: produce a concise executive summary with bullets and “so what”
data_quality: flag OCR uncertainty, missing fields, suspicious numbers
json_mode (optional): for agents that must emit structured JSON blocks
Use a consistent delimiter format so the orchestrator can extract skill sections, e.g.:

## skill: citation_style
When referencing extracted information, cite as: (Source: <filename>, p.<page>).
If page is not known, cite as: (Source: <filename>, page unknown).
Never fabricate page numbers.

## skill: executive_brief
Provide:
1) Executive summary (3–6 bullets)
2) Key findings (grouped)
3) Recommended actions
4) Open questions / missing evidence
7) Prompting Requirements (Non-hallucinatory, Audit-Friendly)
7.1 OCR-to-Markdown prompt (Vision)
The Vision OCR prompt must:

instruct model to transcribe exactly
preserve layout semantically (headings/lists/tables)
output only Markdown
mark unreadable segments with [illegible] rather than guessing
avoid adding commentary
7.2 Markdown transformation prompt (Text-only)
The Markdown rewrite prompt must:

not change meaning
not add facts
fix hyphenation, spacing, headings, table alignment
convert obvious headers into ## / ###
prefer leaving ambiguous structures as plain text rather than inventing a table
7.3 Agent execution prompts
Agent prompt assembly order:

Base system prompt from agent
Inject skill snippets (in consistent order)
Inject global safety rules (“do not fabricate; cite sources; show uncertainty”)
User document context (the Markdown in editor)
User instruction (optional free-text “what do you want?” input)
Output format requirements (Markdown; optional JSON block)
8) Error Handling and Observability
Must implement:

Clear user-facing errors (invalid range, OCR failed, missing poppler, missing keys)
Retries with backoff for LLM calls
Per-step logs viewable in UI (a collapsible “Run Log”)
Status indicators: “Using env key” vs “Using user key”
Guardrails for large requests: warn and offer summarization/truncation controls
9) Hugging Face Spaces Deployment Requirements
Produce files suitable for HF Spaces Streamlit:

app.py (Streamlit entry)
requirements.txt including (at least): streamlit, pypdf or PyMuPDF, PyYAML, pillow, pdf2image, pytesseract, google-generativeai (or Vertex client), openai, plus any pdf viewer dependency you choose.
packages.txt including system deps: tesseract-ocr, poppler-utils
Use HF secrets: read from environment variables. Do not commit secrets.
10) Acceptance Tests (What “Done” Means)
Implement these “definition of done” checks:

PDF preview loads a PDF and displays pages reliably.
Page range parsing correctly interprets 1-3, 5, 9-10 and rejects invalid ranges.
Trim produces a new PDF containing only selected pages (validated by page count).
OCR Local works end-to-end with poppler + Tesseract and returns text.
OCR Vision returns Markdown and preserves at least one table structure in a sample.
Hybrid produces cleaner Markdown than Local OCR alone (visual check).
Keyword highlighting applies without breaking Markdown rendering and can be toggled.
Agent selection lists agents from YAML; prompt editor works; run returns response.
Schema alignment converts a minimally specified YAML to standard schema and app remains functional.
ToC pipeline scans a folder, summarizes first page of each PDF, and generates ToC.md.
API keys: env keys are masked; user keys stored only in session; no disk persistence.
Rate limits: 429 triggers backoff and eventually succeeds/fails gracefully.
History: prior agent runs are accessible and restorable in session.
11) Implementation Notes and Constraints
Prefer reliable libraries and minimal fragile front-end tricks.
Keep heavy processing off the main rerun path; cache what can be cached.
Make it easy to extend: adding a new agent should be as simple as editing YAML and adding skills in SKILL.md.
Avoid vendor lock-in in code: provider layer should be swappable.
Ensure that if a provider is not configured (no key), the UI disables relevant actions and explains how to enable.
12) Deliverables
Generate:

Full Streamlit app source code with modular structure as described.
Default agents.yaml and SKILL.md.
HF Spaces compatible requirements.txt and packages.txt.
README with setup instructions, environment variables, and usage examples.
Minimal unit tests or self-check functions for page range parsing and YAML validation (even if not full pytest suite).
